{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Analytics Data Cleaning & Visualization\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook demonstrates professional-grade data cleaning, transformation, and visualization techniques using real Google Analytics data. It showcases a complete data wrangling pipeline that transforms raw 150,000+ user session events into actionable business intelligence.\n",
        "\n",
        "### Key Objectives:\n",
        "- Clean and validate Google Analytics data\n",
        "- Remove duplicates and handle missing values\n",
        "- Filter bot traffic\n",
        "- Normalize timezones and standardize data\n",
        "- Generate comprehensive data quality reports\n",
        "- Visualize insights and patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load the raw Google Analytics export data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data cleaning class\n",
        "from data_cleaning_analysis import DataCleaningAnalysis\n",
        "\n",
        "# Initialize the data cleaning pipeline\n",
        "cleaner = DataCleaningAnalysis(data_path='data/raw/google_analytics_export.csv')\n",
        "\n",
        "# Load the data\n",
        "df = cleaner.load_data()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initial Data Assessment\n",
        "\n",
        "Assess the quality and structure of the raw data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform initial assessment\n",
        "cleaner.initial_assessment()\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\n=== Data Summary ===\")\n",
        "print(f\"Total Records: {len(df):,}\")\n",
        "print(f\"Total Columns: {len(df.columns)}\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning Pipeline\n",
        "\n",
        "Execute the complete data cleaning pipeline:\n",
        "1. Remove duplicates\n",
        "2. Handle missing values\n",
        "3. Filter bot traffic\n",
        "4. Normalize timezones\n",
        "5. Standardize columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the complete cleaning pipeline\n",
        "cleaner.execute_pipeline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Quality Report\n",
        "\n",
        "Review the comprehensive data quality report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display the quality report\n",
        "quality_report = pd.read_csv('data/processed/data_quality_report.csv')\n",
        "print(\"=== Data Quality Report ===\")\n",
        "print(quality_report.to_string(index=False))\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n=== Quality Summary ===\")\n",
        "print(f\"Total Checks: {len(quality_report)}\")\n",
        "print(f\"Passed: {len(quality_report[quality_report['status'] == 'PASS'])}\")\n",
        "print(f\"Warnings: {len(quality_report[quality_report['status'] == 'WARNING'])}\")\n",
        "print(f\"Failed: {len(quality_report[quality_report['status'] == 'FAIL'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Visualization\n",
        "\n",
        "Create visualizations to explore the cleaned data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned data for visualization\n",
        "cleaned_df = pd.read_csv('data/processed/cleaned_data.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(\"=== Cleaned Data Sample ===\")\n",
        "cleaned_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations if numeric columns exist\n",
        "numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    # Distribution of numeric columns\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols[:4]):\n",
        "        if idx < len(axes):\n",
        "            cleaned_df[col].hist(bins=50, ax=axes[idx], edgecolor='black')\n",
        "            axes[idx].set_title(f'Distribution of {col}')\n",
        "            axes[idx].set_xlabel(col)\n",
        "            axes[idx].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric columns found for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary & Next Steps\n",
        "\n",
        "### Completed:\n",
        "- ✅ Data loaded and assessed\n",
        "- ✅ Duplicates removed\n",
        "- ✅ Missing values handled\n",
        "- ✅ Bot traffic filtered\n",
        "- ✅ Timezones normalized\n",
        "- ✅ Data quality report generated\n",
        "\n",
        "### Next Steps:\n",
        "- Run SQL queries from `outputs/sql_queries/analysis_queries.sql` for deeper analysis\n",
        "- Create Tableau dashboards using the cleaned data\n",
        "- Implement advanced analytics and predictive models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=== Data Cleaning Pipeline Complete ===\")\n",
        "print(f\"Cleaned data saved to: data/processed/cleaned_data.csv\")\n",
        "print(f\"Quality report saved to: data/processed/data_quality_report.csv\")\n",
        "print(f\"\\nFinal Dataset Shape: {cleaned_df.shape}\")\n",
        "print(f\"Total Records: {len(cleaned_df):,}\")\n",
        "print(f\"Total Columns: {len(cleaned_df.columns)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
